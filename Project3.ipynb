{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7195333b-391f-4201-820b-6a08ee99a568",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Jasmine Widgery\n",
    "#Project 3\n",
    "#Fall 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e51be35d-db57-4d08-ac94-3a5cf9a8d513",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Imports\n",
    "import pandas\n",
    "import numpy\n",
    "import math\n",
    "import random\n",
    "import scipy.linalg\n",
    "\n",
    "import warnings\n",
    "\n",
    "#suppress warnings\n",
    "warnings.filterwarnings('ignore') #this suppresses a warning from numpy.exp() that complains about the input being too large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17a0739a-d5c2-468a-b9a5-7ea430f32f9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Step 1. (a) - use linear regression to calculate w\n",
    "\n",
    "#Read in training data\n",
    "\n",
    "#Note-- I prepended the training data in the Excel sheet with a column of 1s\n",
    "trainDF = pandas.read_excel(\"Project3.xlsx\")\n",
    "#print(trainDF)\n",
    "\n",
    "X = numpy.array(trainDF[[\"X0\",\"Midterm\", \"Homework\", \"Quiz\"]])\n",
    "Y = numpy.array(trainDF[\"Course Grade\"])\n",
    "#print(X)\n",
    "#print(Y)\n",
    "\n",
    "\n",
    "XTranspose = numpy.transpose(X)\n",
    "A = numpy.dot(XTranspose, X)\n",
    "b = numpy.dot(XTranspose, Y)\n",
    "\n",
    "w = numpy.dot(numpy.linalg.inv(A), b)\n",
    "#print(w)\n",
    "\n",
    "#Step 1. (b) - use found w to compute predicted grades for testing data\n",
    "\n",
    "#Read in testing data\n",
    "#Note-- I prepended the testing data in the Excel sheet with a column of 1s\n",
    "testDF_lin = pandas.read_excel(\"Project3.xlsx\", sheet_name = \"Predict\")\n",
    "\n",
    "\n",
    "#estimate scores for each point\n",
    "predictedGrades_lin = []\n",
    "for i in testDF_lin.index:\n",
    "    grade = (w[0] * testDF_lin[\"X0\"][i]) + (w[1] * testDF_lin[\"Midterm\"][i]) + (w[2] * testDF_lin[\"Homework\"][i]) + (w[3] * testDF_lin[\"Quiz\"][i])\n",
    "    predictedGrades_lin.append(grade)\n",
    "\n",
    "predictedGrades_lin = numpy.array(predictedGrades_lin)\n",
    "#testDF_lin[\"Predicted Grades-linear regression\"] = predictedGrades_lin\n",
    "#print(testDF_lin)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2580bf19-c07f-4caa-8400-d2664106c823",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final w is: [-0.0048891  -0.05344858  0.01489385  0.07413073]\n",
      "NumIter: 2201\n",
      "    X0  Midterm  Homework  Quiz  Probabilities - logistic regression\n",
      "0    1       49        79    71                             0.978459\n",
      "1    1       56        21    46                             0.673658\n",
      "2    1       58       100    79                             0.985808\n",
      "3    1       61        82    58                             0.905135\n",
      "4    1       62        90    62                             0.932002\n",
      "5    1       66        99    75                             0.970739\n",
      "6    1       71        73    65                             0.891486\n",
      "7    1       73        87    80                             0.965095\n",
      "8    1       73        97    73                             0.950245\n",
      "9    1       74        73    77                             0.944550\n",
      "10   1       74        85    66                             0.900117\n",
      "11   1       74       100    87                             0.981633\n",
      "12   1       78        79    56                             0.760251\n",
      "13   1       80        72    71                             0.886438\n",
      "14   1       80        82    90                             0.973719\n",
      "15   1       81        56    60                             0.720647\n",
      "16   1       81        80    78                             0.933360\n",
      "17   1       82       100    79                             0.950642\n",
      "18   1       83        80    56                             0.711300\n",
      "19   1       83        96    64                             0.849803\n",
      "20   1       84        98    85                             0.963249\n",
      "21   1       84       100    77                             0.937196\n",
      "22   1       85        93    75                             0.916592\n",
      "23   1       85        99    69                             0.885085\n",
      "24   1       86        95    77                             0.925636\n",
      "25   1       86       100    85                             0.960419\n",
      "26   1       87        97    75                             0.912902\n",
      "27   1       87        98    89                             0.967777\n",
      "28   1       87        99    85                             0.957738\n",
      "29   1       87        99    81                             0.943968\n",
      "30   1       88        99    73                             0.898226\n",
      "31   1       89        82    37                             0.310522\n",
      "32   1       89        82    81                             0.921584\n",
      "33   1       89        99    79                             0.928840\n",
      "34   1       90       100    90                             0.965970\n",
      "35   1       91        67    84                             0.913423\n",
      "36   1       91       100    79                             0.922514\n",
      "37   1       92        99    85                             0.945496\n",
      "38   1       93        97    71                             0.849716\n",
      "39   1       94        99    89                             0.954482\n",
      "40   1       95        98    85                             0.935728\n",
      "41   1       95        99    69                             0.818617\n",
      "42   1       97        80    79                             0.865114\n",
      "43   1       97        99    92                             0.957103\n",
      "44   1       97       100    87                             0.939877\n",
      "45   1       98       100    81                             0.904747\n",
      "46   1       99       100   100                             0.973562\n",
      "47   1      100        76    90                             0.920850\n",
      "48   1      100        94    94                             0.953406\n",
      "49   1      100       100    94                             0.957219\n"
     ]
    }
   ],
   "source": [
    "#Step 2. (a) - Calculate course pass/fail for training set\n",
    "\n",
    "pf = []\n",
    "for i in trainDF.index:\n",
    "    if trainDF[\"Course Grade\"][i] < 70:\n",
    "        trainDF[\"Course Grade\"][i] = -1\n",
    "        pf.append(-1)\n",
    "    else:\n",
    "        pf.append(1)\n",
    "        trainDF[\"Course Grade\"][i] = 1\n",
    "        \n",
    "pf = numpy.array(pf)\n",
    "Y = numpy.array(trainDF[\"Course Grade\"])\n",
    "\n",
    "#Step 2. (b) - Run logistic regression algorithm to compute w\n",
    "def CloseToZero(g):\n",
    "    numClose = 0\n",
    "    for i in range(0, len(g)):\n",
    "        if math.isclose(g[i], 0, abs_tol = 0.01):\n",
    "            #print(\"G[i] = \" + str(g[i]))\n",
    "            numClose += 1  \n",
    "                   \n",
    "    if numClose == len(g):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "N = len(Y)\n",
    "MAX = N - 1\n",
    "global wFinal\n",
    "global numIter\n",
    "numIter = 0\n",
    "def logisticRegressionStochastic(w, eta):\n",
    "    global wFinal\n",
    "    global numIter\n",
    "    #choose random D(x, y)\n",
    "    index = random.randint(0, MAX)\n",
    "    randX = X[index]\n",
    "    randX = numpy.array(randX)\n",
    "    randY = Y[index]\n",
    "    #print(\"Point: \" + str(randX) + \" , \" + str(randY))\n",
    "    \n",
    "    num = -randY * randX\n",
    "    numToExp = randY * numpy.dot(w, randX)\n",
    "    denom = 1 + numpy.exp(numToExp)\n",
    "    gradient = num / denom\n",
    "    gradient = numpy.array(gradient)\n",
    "    \n",
    "    wNext = w - (eta * gradient)\n",
    "    \n",
    "    #print(\"Gradient: \" + str(gradient))\n",
    "    #print(\"W: \" + str(wNext))\n",
    "    if CloseToZero(gradient) or numIter > 2200:\n",
    "        wFinal = wNext\n",
    "        print(\"Final w is: \" + str(wNext))\n",
    "        #print(\"Final g is: \" + str(gradient))\n",
    "        print(\"NumIter: \" + str(numIter))\n",
    "        return wNext\n",
    "    else:\n",
    "        numIter += 1\n",
    "        logisticRegressionStochastic(wNext, eta)\n",
    "\n",
    "w = [0, 0, 0, 0]                       \n",
    "logisticRegressionStochastic(w, 0.00008)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + numpy.exp(-x))\n",
    "\n",
    "\n",
    "\n",
    "#Step 2. (c) - using found weights, calculate probability for each point of passing the course\n",
    "w = wFinal\n",
    "probs = []\n",
    "for i in testDF_lin.index:\n",
    "    prob = (w[0] * testDF_lin[\"X0\"][i]) + (w[1] * testDF_lin[\"Midterm\"][i]) + (w[2] * testDF_lin[\"Homework\"][i]) + (w[3] * testDF_lin[\"Quiz\"][i])\n",
    "    prob = sigmoid(prob)\n",
    "    probs.append(prob)\n",
    "    \n",
    "probs = numpy.array(probs)\n",
    "testDF_lin[\"Probabilities - logistic regression\"] = probs\n",
    "print(testDF_lin)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39eff1ee-f167-4c1b-887c-28665277d65f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
